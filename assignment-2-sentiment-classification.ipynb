{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Shashank Suroju\n# 19CS10061","metadata":{}},{"cell_type":"markdown","source":"## Importing Libraries","metadata":{}},{"cell_type":"code","source":"import os\nimport sys\nimport shutil\nfrom tqdm import tqdm\n\nimport numpy as np \nimport pandas as pd \n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import TensorDataset, DataLoader\n\nimport re\nimport string\nimport gensim\nfrom nltk.corpus import stopwords \nfrom collections import Counter\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set()\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score, accuracy_score","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-09-15T09:29:09.083086Z","iopub.execute_input":"2022-09-15T09:29:09.083407Z","iopub.status.idle":"2022-09-15T09:29:09.097830Z","shell.execute_reply.started":"2022-09-15T09:29:09.083374Z","shell.execute_reply":"2022-09-15T09:29:09.096645Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## Initializing Constants","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\") \n\nclasses_dict = {\n    \"negative\" : 0,\n    \"positive\" : 1\n}\n\ndf[\"sentiment\"] = df[\"sentiment\"].map(lambda x : classes_dict[x])\ndf.head()","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2022-09-15T09:29:09.100602Z","iopub.execute_input":"2022-09-15T09:29:09.101323Z","iopub.status.idle":"2022-09-15T09:29:09.654021Z","shell.execute_reply.started":"2022-09-15T09:29:09.101284Z","shell.execute_reply":"2022-09-15T09:29:09.653221Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"                                              review  sentiment\n0  One of the other reviewers has mentioned that ...          1\n1  A wonderful little production. <br /><br />The...          1\n2  I thought this was a wonderful way to spend ti...          1\n3  Basically there's a family where a little boy ...          0\n4  Petter Mattei's \"Love in the Time of Money\" is...          1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>One of the other reviewers has mentioned that ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I thought this was a wonderful way to spend ti...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Basically there's a family where a little boy ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"config = dict(\n        max_seq_len = 500,\n        embedding_dim = 64,\n        output_dim = 1,\n        hidden_dim = 256,\n        no_layers = 2,\n        batch_size = 32,\n        epochs = 40,\n        num_classes = 2,\n        seed = 2022,\n        learning_rate = 0.0001,\n        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    )","metadata":{"execution":{"iopub.status.busy":"2022-09-15T09:29:09.655499Z","iopub.execute_input":"2022-09-15T09:29:09.655917Z","iopub.status.idle":"2022-09-15T09:29:09.663761Z","shell.execute_reply.started":"2022-09-15T09:29:09.655874Z","shell.execute_reply":"2022-09-15T09:29:09.662980Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"## Data Splitting","metadata":{}},{"cell_type":"code","source":"X,y = df['review'].values, df['sentiment'].values\n\nX_train,X_val,y_train,y_val = train_test_split(X,y,train_size=0.8,stratify=y,random_state=config[\"seed\"])\nX_val,X_test,y_val,y_test = train_test_split(X_val,y_val,train_size=0.5,stratify=y_val,random_state=config[\"seed\"])\n\nprint(f\"Number of Samples in Training data : {len(X_train)}\")\nprint(f\"Number of Samples in Validation data : {len(X_val)}\")\nprint(f\"Number of Samples in Training data : {len(X_test)}\")","metadata":{"execution":{"iopub.status.busy":"2022-09-15T09:29:09.666729Z","iopub.execute_input":"2022-09-15T09:29:09.667332Z","iopub.status.idle":"2022-09-15T09:29:09.703405Z","shell.execute_reply.started":"2022-09-15T09:29:09.667295Z","shell.execute_reply":"2022-09-15T09:29:09.702614Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Number of Samples in Training data : 40000\nNumber of Samples in Validation data : 5000\nNumber of Samples in Training data : 5000\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Tokenization","metadata":{}},{"cell_type":"code","source":"SOS=\"<s>\"\nEOS=\"</s>\"\nUNK=\"<UNK>\"\n\ndef add_sentence_tokens(sentences):\n    return ['{} {} {}'.format(SOS, s, EOS) for s in sentences]\n\ndef replace_singletons(tokens):\n    vocab = Counter([token for token in tokens])\n    return [token if vocab[token] > 1 else UNK for token in tokens]\n\ndef preprocess_word(s):\n    if s not in [SOS, EOS, UNK]:\n        s = re.sub(r\"[^\\w\\s]\", '', s) # Remove all non-word characters (everything except numbers and letters)\n        s = re.sub(r\"\\s+\", '', s) # Replace all runs of whitespaces with no space\n        s = re.sub(r\"\\d\", '', s) # replace digits with no space\n        s = s.lower()\n    return s\n\ndef preprocess_reviews(sentences):\n    sentences = add_sentence_tokens(sentences)\n    tokens = ' '.join(sentences).split(' ')\n    tokens = replace_singletons(tokens)\n\n    ans=[]\n    temp=[]\n    \n    stop_words = set(stopwords.words('english')) \n    \n    for token in tokens:\n        token = preprocess_word(token)\n        #if token not in stop_words:\n        temp.append(token)\n        if token == EOS:\n            ans.append(temp)\n            temp=[]\n    return ans\n\n\ntrain_strings = preprocess_reviews(X_train)\n#train_strings = [\" \".join(i) for i in train_strings]\nword_model = gensim.models.Word2Vec(train_strings, size=config[\"embedding_dim\"], min_count=1, window=5)\npretrained_weights = word_model.wv.vectors\nvocab_size, emdedding_size = pretrained_weights.shape\n\nconfig[\"vocab_size\"] = vocab_size\nprint(f\"Vocabulary Size: {vocab_size}\")\n\ndef word2idx(word):\n    try:\n        return word_model.wv.vocab[word].index\n    except:\n        return word_model.wv.vocab[UNK].index\n    \ndef idx2word(idx):\n    return word_model.wv.index2word[idx]\n\ndef tokenize(reviews):\n    tokens = preprocess_reviews(reviews)\n    sequences = np.zeros([len(reviews), config[\"max_seq_len\"]], dtype=np.int32)\n    \n    for i,review in tqdm(enumerate(tokens)):\n        for j,word in enumerate(review[:config[\"max_seq_len\"]]):    \n            sequences[i,j] = word2idx(word)\n    \n    return sequences","metadata":{"execution":{"iopub.status.busy":"2022-09-15T09:29:09.705311Z","iopub.execute_input":"2022-09-15T09:29:09.705786Z","iopub.status.idle":"2022-09-15T09:31:04.126648Z","shell.execute_reply.started":"2022-09-15T09:29:09.705748Z","shell.execute_reply":"2022-09-15T09:31:04.125816Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Vocabulary Size: 61909\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Data Loader","metadata":{}},{"cell_type":"code","source":"X_train = tokenize(X_train)\nX_val = tokenize(X_val)\nX_test = tokenize(X_test)\n\n# create Tensor datasets\ntrain_data = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\nval_data = TensorDataset(torch.from_numpy(X_val), torch.from_numpy(y_val))\ntest_data = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))\n\n# DataLoader\ntrain_loader = DataLoader(train_data, shuffle=True, batch_size=config[\"batch_size\"])\nvalid_loader = DataLoader(val_data, shuffle=False, batch_size=config[\"batch_size\"])\ntest_loader = DataLoader(test_data, shuffle=False, batch_size=config[\"batch_size\"])","metadata":{"execution":{"iopub.status.busy":"2022-09-15T09:31:04.127930Z","iopub.execute_input":"2022-09-15T09:31:04.128347Z","iopub.status.idle":"2022-09-15T09:32:04.868154Z","shell.execute_reply.started":"2022-09-15T09:31:04.128310Z","shell.execute_reply":"2022-09-15T09:32:04.867356Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"40000it [00:05, 6761.74it/s]\n5000it [00:00, 7245.56it/s]\n5000it [00:00, 7025.60it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"# obtain one batch of training data\ndataiter = iter(train_loader)\nsample_x, sample_y = dataiter.next()\n\nprint('Sample input size: ', sample_x.size())\nprint('Sample input: \\n', sample_x)\nprint('Sample input: \\n', sample_y)","metadata":{"execution":{"iopub.status.busy":"2022-09-15T09:32:04.869498Z","iopub.execute_input":"2022-09-15T09:32:04.869960Z","iopub.status.idle":"2022-09-15T09:32:04.889096Z","shell.execute_reply.started":"2022-09-15T09:32:04.869921Z","shell.execute_reply":"2022-09-15T09:32:04.888204Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Sample input size:  torch.Size([32, 500])\nSample input: \n tensor([[   30,    33,     0,  ...,     0,     0,     0],\n        [   30, 38005,    26,  ...,     0,     0,     0],\n        [   30,    46,    23,  ...,     0,     0,     0],\n        ...,\n        [   30,     9,    14,  ...,     0,     0,     0],\n        [   30,  1258,  1509,  ...,     0,     0,     0],\n        [   30,    46,    36,  ...,     0,     0,     0]], dtype=torch.int32)\nSample input: \n tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n        1, 0, 1, 0, 0, 1, 1, 0])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Classifier Model","metadata":{}},{"cell_type":"code","source":"class SentimentClassifier(nn.Module):\n    def __init__(self,no_layers,vocab_size,hidden_dim,embedding_dim,output_dim,drop_prob=0.3):\n        super(SentimentClassifier,self).__init__()\n \n        self.output_dim = output_dim\n        self.hidden_dim = hidden_dim\n        self.no_layers = no_layers\n        self.vocab_size = vocab_size\n    \n        self.embedding = nn.Embedding(vocab_size, embedding_dim) # embedding and LSTM layers\n        self.embedding.weight.data.copy_(torch.from_numpy(pretrained_weights)) # Initializing with Word2Vec \n        \n        #Bidirectional LSTM\n        self.lstm = nn.LSTM(input_size=embedding_dim,hidden_size=self.hidden_dim,\n                           num_layers=no_layers, batch_first=True, bidirectional=True)\n        \n        \n        # dropout layer\n        self.dropout = nn.Dropout(0.3)\n    \n        # linear and sigmoid layer\n        self.fc = nn.Linear(self.hidden_dim, self.output_dim)\n        self.classifier = nn.Sigmoid()\n        \n    def forward(self,x,hidden):\n        batch_size = x.size(0)\n        embeds = self.embedding(x)  # shape: B x S x Feature   since batch = True\n        \n        lstm_out, hidden = self.lstm(embeds) # dropout and fully connected layer\n        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim) \n        \n        out = self.dropout(lstm_out) # dropout and fully connected layer\n        out = self.fc(out)\n        \n        sig_out = self.classifier(out) # sigmoid function\n        sig_out = sig_out.view(batch_size, -1) # reshape to be batch_size first\n        sig_out = sig_out[:, -1] # get last batch of labels\n        \n        return sig_out, hidden\n        \n    def init_hidden(self, batch_size):\n        ''' \n        Initializes hidden state \n        Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n        Initialized to zero, for hidden state and cell state of LSTM\n        '''\n        h0 = torch.zeros((self.no_layers,config[\"batch_size\"],self.hidden_dim)).to(config[\"device\"])\n        c0 = torch.zeros((self.no_layers,config[\"batch_size\"],self.hidden_dim)).to(config[\"device\"])\n        hidden = (h0,c0)\n        return hidden","metadata":{"execution":{"iopub.status.busy":"2022-09-15T09:32:04.890684Z","iopub.execute_input":"2022-09-15T09:32:04.890965Z","iopub.status.idle":"2022-09-15T09:32:04.906839Z","shell.execute_reply.started":"2022-09-15T09:32:04.890931Z","shell.execute_reply":"2022-09-15T09:32:04.905986Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"## The Model","metadata":{}},{"cell_type":"code","source":"model = SentimentClassifier(config[\"no_layers\"],config[\"vocab_size\"],config[\"hidden_dim\"],config[\"embedding_dim\"],config[\"output_dim\"],drop_prob=0.3)\nprint(model)\nmodel.to(config[\"device\"])\ncriterion = nn.BCELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=config[\"learning_rate\"])","metadata":{"execution":{"iopub.status.busy":"2022-09-15T09:32:04.908745Z","iopub.execute_input":"2022-09-15T09:32:04.909483Z","iopub.status.idle":"2022-09-15T09:32:04.975091Z","shell.execute_reply.started":"2022-09-15T09:32:04.909442Z","shell.execute_reply":"2022-09-15T09:32:04.974184Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"SentimentClassifier(\n  (embedding): Embedding(61909, 64)\n  (lstm): LSTM(64, 256, num_layers=2, batch_first=True, bidirectional=True)\n  (dropout): Dropout(p=0.3, inplace=False)\n  (fc): Linear(in_features=256, out_features=1, bias=True)\n  (classifier): Sigmoid()\n)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"epoch_tr_loss,epoch_vl_loss = [],[]\nepoch_tr_acc,epoch_vl_acc = [],[]\nepoch_tr_f1, epoch_vl_f1 = [], []\nval_best_f1 = -1*np.inf\n\ndevice = config[\"device\"]\nfor epoch in range(config[\"epochs\"]):\n    train_loss, total_instances = 0, 0\n    y_true, y_pred = [], []\n    model.train()\n    # initialize hidden state \n    h = model.init_hidden(config[\"batch_size\"])\n    for inputs, labels in train_loader:\n        \n        inputs, labels = inputs.to(device, dtype=torch.long), labels.to(device)   \n        h = tuple([each.data for each in h])\n        \n        model.zero_grad()\n        output,h = model(inputs,h)\n        \n        loss = criterion(output.squeeze(), labels.float())\n        loss.backward()\n        \n        num_instances = labels.size(0)\n        train_loss += (loss.item()*num_instances)\n        total_instances += num_instances\n        \n        y_true.append(labels.cpu().detach().numpy())\n        y_pred.append([1 if i>=0.5 else 0 for i in output.cpu().detach().numpy()])\n        \n        optimizer.step()\n    \n    train_loss = train_loss/total_instances\n    y_true = np.concatenate(y_true)\n    y_pred = np.concatenate(y_pred)\n    \n    train_acc = accuracy_score(y_true,y_pred)\n    train_f1 = f1_score(y_true,y_pred)\n    \n    val_h = model.init_hidden(config[\"batch_size\"])\n    val_loss, total_instances = 0, 0\n    y_true, y_pred = [], []\n    model.eval()\n    for inputs, labels in valid_loader:\n            val_h = tuple([each.data for each in val_h])\n\n            inputs, labels = inputs.to(device, dtype=torch.long), labels.to(device)\n\n            output, val_h = model(inputs,h)\n            \n            val_loss = criterion(output.squeeze(), labels.float())\n            num_instances = labels.size(0)\n            val_loss += (loss.item()*num_instances)\n            total_instances += num_instances\n            \n            y_true.append(labels.cpu().detach().numpy())\n            y_pred.append([1 if i>=0.5 else 0 for i in output.cpu().detach().numpy()])\n    \n    val_loss = val_loss/total_instances\n    y_true = np.concatenate(y_true)\n    y_pred = np.concatenate(y_pred)\n    \n    val_acc = accuracy_score(y_true,y_pred)\n    val_f1 = f1_score(y_true,y_pred)\n    \n    epoch_tr_loss.append(train_loss)\n    epoch_vl_loss.append(val_loss)\n    epoch_tr_acc.append(train_acc)\n    epoch_vl_acc.append(val_acc)\n    epoch_tr_f1.append(train_f1)\n    epoch_vl_f1.append(val_f1)\n    \n    print(f'Epoch {epoch+1}') \n    print(f'Train_loss : {train_loss} || Val_loss : {val_loss}')\n    print(f'Train_accuracy : {train_acc*100} || Val_accuracy : {val_acc*100}')\n    print(f'Train_f1_score : {train_f1*100} || Val_f1_score : {val_f1*100}')\n    if val_f1 >= val_best_f1:\n        torch.save(model.state_dict(), './best_model.pt')\n        print('Validation F1 loss increased ({:.6f} --> {:.6f}). Saving model ...'.format(val_best_f1,val_f1))\n        val_best_f1 = val_f1\n    print(25*'==')","metadata":{"execution":{"iopub.status.busy":"2022-09-15T09:32:04.976420Z","iopub.execute_input":"2022-09-15T09:32:04.976838Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1\nTrain_loss : 0.6932062998771668 || Val_loss : 0.0012452338123694062\nTrain_accuracy : 50.372499999999995 || Val_accuracy : 50.03999999999999\nTrain_f1_score : 49.18989480150503 || Val_f1_score : 66.55061596143545\nValidation F1 loss increased (-inf --> 0.665506). Saving model ...\n==================================================\nEpoch 2\nTrain_loss : 0.6922527379512787 || Val_loss : 0.0012467908672988415\nTrain_accuracy : 50.795 || Val_accuracy : 50.63999999999999\nTrain_f1_score : 43.25664533241077 || Val_f1_score : 66.14540466392319\n==================================================\nEpoch 3\nTrain_loss : 0.6918382560253143 || Val_loss : 0.001244785264134407\nTrain_accuracy : 50.81250000000001 || Val_accuracy : 50.760000000000005\nTrain_f1_score : 45.95819485263823 || Val_f1_score : 15.337001375515818\n==================================================\nEpoch 4\nTrain_loss : 0.6897377736568451 || Val_loss : 0.0012352577177807689\nTrain_accuracy : 51.082499999999996 || Val_accuracy : 51.06\nTrain_f1_score : 44.690052859202304 || Val_f1_score : 66.88320476383814\nValidation F1 loss increased (0.665506 --> 0.668832). Saving model ...\n==================================================\nEpoch 5\nTrain_loss : 0.6873364785671234 || Val_loss : 0.001251895329914987\nTrain_accuracy : 51.470000000000006 || Val_accuracy : 50.660000000000004\nTrain_f1_score : 47.95152295152295 || Val_f1_score : 15.715749914588317\n==================================================\nEpoch 6\nTrain_loss : 0.6901757259845733 || Val_loss : 0.0012321375543251634\nTrain_accuracy : 51.044999999999995 || Val_accuracy : 51.25999999999999\nTrain_f1_score : 47.1099827139153 || Val_f1_score : 66.61186463899165\n==================================================\nEpoch 7\nTrain_loss : 0.6887480369567871 || Val_loss : 0.0012440371792763472\nTrain_accuracy : 51.165000000000006 || Val_accuracy : 51.160000000000004\nTrain_f1_score : 38.50657936158157 || Val_f1_score : 8.948545861297537\n==================================================\nEpoch 8\nTrain_loss : 0.6884636102199554 || Val_loss : 0.0011639554286375642\nTrain_accuracy : 51.81249999999999 || Val_accuracy : 66.52\nTrain_f1_score : 53.00958092591237 || Val_f1_score : 66.18181818181817\n==================================================\nEpoch 9\nTrain_loss : 0.4346453062415123 || Val_loss : 0.0008510201005265117\nTrain_accuracy : 80.7675 || Val_accuracy : 85.36\nTrain_f1_score : 80.9546208501473 || Val_f1_score : 85.88507520246817\nValidation F1 loss increased (0.668832 --> 0.858851). Saving model ...\n==================================================\nEpoch 10\nTrain_loss : 0.31799912489056587 || Val_loss : 0.0008228335063904524\nTrain_accuracy : 86.735 || Val_accuracy : 86.6\nTrain_f1_score : 86.80493385059187 || Val_f1_score : 86.1053504769805\nValidation F1 loss increased (0.858851 --> 0.861054). Saving model ...\n==================================================\nEpoch 11\nTrain_loss : 0.2899578289955854 || Val_loss : 0.0007150661549530923\nTrain_accuracy : 87.81750000000001 || Val_accuracy : 87.64\nTrain_f1_score : 87.85060709566432 || Val_f1_score : 87.23140495867769\nValidation F1 loss increased (0.861054 --> 0.872314). Saving model ...\n==================================================\nEpoch 12\nTrain_loss : 0.2733118537425995 || Val_loss : 0.0005381829105317593\nTrain_accuracy : 88.725 || Val_accuracy : 87.88\nTrain_f1_score : 88.75199521149243 || Val_f1_score : 87.99048751486328\nValidation F1 loss increased (0.872314 --> 0.879905). Saving model ...\n==================================================\nEpoch 13\nTrain_loss : 0.25685428646206854 || Val_loss : 0.0005696440930478275\nTrain_accuracy : 89.5725 || Val_accuracy : 88.46000000000001\nTrain_f1_score : 89.58837772397096 || Val_f1_score : 88.56293359762141\nValidation F1 loss increased (0.879905 --> 0.885629). Saving model ...\n==================================================\nEpoch 14\nTrain_loss : 0.2430385270446539 || Val_loss : 0.00044717834680341184\nTrain_accuracy : 90.21249999999999 || Val_accuracy : 88.72\nTrain_f1_score : 90.22252191503709 || Val_f1_score : 88.70192307692307\nValidation F1 loss increased (0.885629 --> 0.887019). Saving model ...\n==================================================\nEpoch 15\nTrain_loss : 0.22888125787079333 || Val_loss : 0.0006966625223867595\nTrain_accuracy : 90.975 || Val_accuracy : 88.8\nTrain_f1_score : 90.9822142286171 || Val_f1_score : 88.65937626569462\n==================================================\nEpoch 16\nTrain_loss : 0.21352470514178276 || Val_loss : 0.0004566910502035171\nTrain_accuracy : 91.59 || Val_accuracy : 88.74\nTrain_f1_score : 91.60175753944478 || Val_f1_score : 88.46547838557672\n==================================================\nEpoch 17\nTrain_loss : 0.1999474722802639 || Val_loss : 0.00037301392876543105\nTrain_accuracy : 92.3075 || Val_accuracy : 88.92\nTrain_f1_score : 92.31767907522533 || Val_f1_score : 89.1965678627145\nValidation F1 loss increased (0.887019 --> 0.891966). Saving model ...\n==================================================\nEpoch 18\nTrain_loss : 0.1877663458660245 || Val_loss : 0.00021433843357954174\nTrain_accuracy : 92.83500000000001 || Val_accuracy : 88.86\nTrain_f1_score : 92.84287283987614 || Val_f1_score : 88.52729145211123\n==================================================\nEpoch 19\nTrain_loss : 0.17027971964925528 || Val_loss : 0.00017367259715683758\nTrain_accuracy : 93.635 || Val_accuracy : 88.03999999999999\nTrain_f1_score : 93.63945238333167 || Val_f1_score : 88.6527514231499\n==================================================\nEpoch 20\nTrain_loss : 0.15673745398446917 || Val_loss : 0.00039388699224218726\nTrain_accuracy : 94.2325 || Val_accuracy : 88.94\nTrain_f1_score : 94.23984419864672 || Val_f1_score : 89.08193484698914\n==================================================\nEpoch 21\nTrain_loss : 0.1431237292177975 || Val_loss : 0.00026167233590967953\nTrain_accuracy : 94.7675 || Val_accuracy : 88.75999999999999\nTrain_f1_score : 94.76867704766427 || Val_f1_score : 88.67848509266719\n==================================================\nEpoch 22\nTrain_loss : 0.124948674762249 || Val_loss : 0.00021211110288277268\nTrain_accuracy : 95.56750000000001 || Val_accuracy : 88.22\nTrain_f1_score : 95.57225982069276 || Val_f1_score : 88.65343864380658\n==================================================\nEpoch 23\nTrain_loss : 0.10907598433680832 || Val_loss : 0.0003463992616161704\nTrain_accuracy : 96.2825 || Val_accuracy : 88.9\nTrain_f1_score : 96.2851932349047 || Val_f1_score : 88.80822746521476\n==================================================\nEpoch 24\nTrain_loss : 0.09715002592615783 || Val_loss : 0.00022113656450528651\nTrain_accuracy : 96.76249999999999 || Val_accuracy : 88.82\nTrain_f1_score : 96.76258093547662 || Val_f1_score : 88.98522167487684\n==================================================\nEpoch 25\nTrain_loss : 0.08479514867290855 || Val_loss : 0.00014603303861804307\nTrain_accuracy : 97.13000000000001 || Val_accuracy : 88.6\nTrain_f1_score : 97.13014349282535 || Val_f1_score : 88.81475667189953\n==================================================\nEpoch 26\nTrain_loss : 0.07253997196033597 || Val_loss : 0.0004043421649839729\nTrain_accuracy : 97.68 || Val_accuracy : 88.78\nTrain_f1_score : 97.68081171589942 || Val_f1_score : 88.92398815399802\n==================================================\nEpoch 27\nTrain_loss : 0.060335890843346715 || Val_loss : 0.0002009374147746712\nTrain_accuracy : 98.1025 || Val_accuracy : 88.46000000000001\nTrain_f1_score : 98.10273715785527 || Val_f1_score : 88.7546287273436\n==================================================\nEpoch 28\nTrain_loss : 0.05024048991659656 || Val_loss : 0.00023252281243912876\nTrain_accuracy : 98.5525 || Val_accuracy : 88.6\nTrain_f1_score : 98.55202940955809 || Val_f1_score : 88.45218800648298\n==================================================\nEpoch 29\nTrain_loss : 0.04511280323583633 || Val_loss : 0.0005271169357001781\nTrain_accuracy : 98.68 || Val_accuracy : 88.84\nTrain_f1_score : 98.68052778888445 || Val_f1_score : 89.06311250490005\n==================================================\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Training History","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize = (21, 6))\nplt.subplot(1, 3, 1)\nplt.plot(epoch_tr_loss, label='Train loss')\nplt.plot(epoch_vl_loss, label='Validation loss')\nplt.title(\"Loss\")\nplt.legend()\nplt.grid()\n    \nplt.subplot(1, 3, 2)\nplt.plot(epoch_tr_acc, label='Train Accuracy')\nplt.plot(epoch_vl_acc, label='Validation Accuracy')\nplt.title(\"Accuracy\")\nplt.legend()\nplt.grid()\n\nplt.subplot(1, 3, 3)\nplt.plot(epoch_tr_f1, label='Train F1 Score')\nplt.plot(epoch_vl_f1, label='Validation F1 Score')\nplt.title(\"F1 Score\")\nplt.legend()\nplt.grid()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inference","metadata":{}},{"cell_type":"code","source":"model.load_state_dict(torch.load(\"../input/best-model/Best_train_model.pt\"))\n\ny_true, y_pred = [], []\nmodel.eval()\nfor inputs, labels in test_loader:\n        test_h = tuple([each.data for each in val_h])\n\n        inputs, labels = inputs.to(device, dtype=torch.long), labels.to(device)\n        output, test_h = model(inputs,test_h)\n            \n        y_true.append(labels.cpu().detach().numpy())\n        y_pred.append([1 if i>=0.5 else 0 for i in output.cpu().detach().numpy()])\n            \ny_true = np.concatenate(y_true)\ny_pred = np.concatenate(y_pred) \n\ntest_acc = accuracy_score(y_true,y_pred)\ntest_f1 = f1_score(y_true,y_pred)\n\nprint(f\"Test Accuracy : {test_acc*100} || Test F1 Score: {test_f1*100}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Got Test Accuracy : 89.4 \n## Test F1 Score: 89.34459187776437","metadata":{}},{"cell_type":"markdown","source":"# [Best_trained_model](https://drive.google.com/file/d/1ks0rDTMqAUCnBFqpkfCYgNoj-3tUjGRm/view?usp=sharing)","metadata":{}},{"cell_type":"code","source":"plt.plot(y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}